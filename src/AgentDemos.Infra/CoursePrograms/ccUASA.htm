<html xmlns:mso="urn:schemas-microsoft-com:office:office" xmlns:msdt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<title>Data Engineering with Azure Synapse Analytics</title>
	<meta name="NumberOfDays" content="5">

	<!--[if gte mso 9]><xml>
<mso:CustomDocumentProperties>
<mso:ContentTypeId msdt:dt="string">0x0101009C33340F38E14D1EABEAC83093A8F1720026EC9B8D561B9E4FB2429BA5E01CB4D7</mso:ContentTypeId>
</mso:CustomDocumentProperties>
</xml><![endif]-->

	<meta id="ConnectiveDocSignExtentionInstalled" name="ConnectiveDocSignExtentionInstalled"
		data-extension-version="1.0.6">
</head>

<body>
	<h2>Learning Goals</h2>

	<p>
		Handling large volumes of data requires different skills: One must master storage options, tools to upload data
		performant, handling failed uploads, and convert data in a format appropriate for reporting and analysis.
		In the Microsoft Azure stack, Synapse Analytics is the cornerstone service for the data engineer. It encompasses
		pipelines to copy data, Spark and SQL to transform and query data, Data Explorer for near real-time analysis and
		data exploration, and Power BI for reporting.
	</p>
	<p>
		This training teaches how to use Synapse Analytics to design, build and maintain a modern data lake
		architecture. The
		training also includes a few other Azure services which come in handy when working with Synapse Analytics, such
		as Azure Data Vault for handling authentication and Azure SQL Database for dealing with smaller datasets.
	</p>

	<h2>Target Audience</h2>
	<p>This course focusses on developers and administrators who are considering migrating existing
		data solutions to the Microsoft Azure cloud or start designing new data oriented solutions in the Azure cloud.
		Some familiarity with relational database
		systems such as SQL Server is handy. Prior knowledge of Azure is not required.
	</p>

	<h2>Course Outline</h2>


	<h4>The Modern Data Warehouse</h4>

	<p>
		The cloud requires to reconsider some of the choices made for on-premisses data handling. This module
		introduces the concept of a data lake and the data lakehouse. It also introduces the different services in Azure
		that can be used for data processing and compares them to the
		traditional on-premisses data stack. Finally, it provides a brief intro in Azure and the use of the Azure
		portal.
	</p>

	<ul>
		<li>From traditional to modern data warehouse</li>
		<li>Comparing data warehouse with data lake</li>
		<li>Lambda architecture</li>
		<li>Overview of Big Data related Azure services</li>
		<li>Getting started with the Azure Portal</li>
		<li>LAB: Navigating the Azure Portal</li>
	</ul>

	<h4>Storing Data in Azure</h4>

	<p>
		This module discusses the different types of storage available in Azure Storage as well as data
		lake storage. Also, some of the tools to load and manage files in Azure storage and Data lake storage are
		covered.
	</p>

	<ul>
		<li>Introduction Azure Storage</li>
        <li>Comparison of Azure Blob Storage vs Azure Data Lake Storage Gen2</li>
        <li>Create storage account and containers</li>
		<li>Entra ID identification versus key based authentication</li>
		<li>Folders and folder level permissions in Data Lake Gen 2</li>
        <li>Working with the Azure Storage Explorer and AzCopy</li>
        <li>LAB: Uploading data into Azure Storage</li>
	</ul>

	<h4>Overview of Azure Synapse Analytics</h4>
	<p>
		Synapse Analytics is the cornerstone service for the data engineer. It encompasses
		pipelines to copy data, Spark and SQL to transform and query data, Data Explorer for near real-time analysis and
		data exploration and Power BI for reporting. This module provides a brief introduction into this service.
	</p>

	<ul>
		<li>The different components of Synapse Analytics</li>
		<li>Provisioning a Synapse Analytics workspace</li>
		<li>GitHub setup</li>
		<li>Navigating the Synapse Analytics Studio</li>
		<li>LAB: Provision an Azure Synapse Analytics workspace</li>
	</ul>

	<h4>Introduction to Azure Synapse Analytics Pipelines</h4>

	<p>
		When the data is stored on-premises, you typically use ETL tools such as SQL Server Integration Services for
		loading and transforming data. But what if the data is stored in the Azure cloud? Then you can use pipelines in
		Azure Synapse Analytics.
		This service is nearly identical to Azure Data Factory, the cloud based ETL service. First, we need to
		get used to the terminology, then we can start creating the proper objects in the portal.
	</p>

	<ul>
		<li>Introducing Azure Synapse Analytics Pipelines</li>
		<li>Pipeline terminology</li>
		<li>Creating Pipelines, Linked Services and Datasets</li>
		<li>Copying data with the Copy Data wizard</li>
		<li>LAB: Migrating data with Data Factory Wizard</li>
	</ul>

	<h4>Authoring Pipelines in Azure Synapse Analytics</h4>

	<p>
		This module dives deeper into the process of building an Azure Synapse pipeline. The most common activities
		are illustrated. The module also focusses on how
		to work with expressions, variables and parameters to make the pipelines more dynamic.
	</p>

	<ul>
		<li>Adding activities to the pipeline</li>
		<li>Working with Expressions</li>
		<li>Variables and Parameters</li>
		<li>Debugging a pipeline</li>
		<li>LAB: Authoring and debugging an ASA pipeline</li>
	</ul>

	<h4>Authoring Data Flows in Azure Synapse Analytics</h4>

	<p>
		With Data flows you can visually design data transformations without the need to learn yet another tool or language (such as Databricks or
        Spark). 
	</p>
	<ul>
		<li>From ELT to ETL</li>
		<li>Creating Data flows</li>
		<li>Sizing the Spark cluster</li>
		<li>Running and Profiling Data flows</li>
		<li>LAB: Transforming data using Data Flows</li>
	</ul>

	<h4>Managing Synapse Pipelines</h4>
	<p>
		Pipelines need an integration runtime to control where the code executes. This module provides an overview of the 3
		types of Integration Runtimes: Azure, self-hosted runtimes and SSIS. 
		It also discusses the different type of Triggers that exist and how they can be used to schedule pipelines.
	</p>
	<ul>
		<li>Integration Runtime Overview</li>
		<li>The Azure Integration Runtime</li>
		<li>The Self-Hosted Integration Runtime</li>
		<li>Scheduling Pipelines using Triggers</li>
		<li>Monitoring pipeline executions</li>
	</ul>

	<h4>Azure SQL Database</h4>
	<p>
		An easy way to create a business intelligence solution in the cloud is by taking SQL Server -- familiar to
		many Microsoft
		BI developers -- and run it in the cloud. Backup and high availability happen automatically, and we can use
		nearly
		all the skills and tools we used on a local SQL Server on this cloud based solution as well.
	</p>
	<ul>
		<li>Provisioning an Azure SQL Database</li>
		<li>Migrating an on-premisses Data Warehouse to Azure SQL Database</li>
		<li>Ingesting Azure Blob Storage data</li>
		<li>Working with Columnstore Indexes</li>
		<li>LAB: Using Azure SQL Databases</li>
	</ul>

	<h4>Using the Serverless SQL pool in Azure Synapse Analytics</h4>
	<p>
		Once data has been loaded into the data lake, the next step is to cleanse the data, pre-aggregate the data and
		perform other steps to make the data accessible to reporting and analytical tools.
		Dependant on the transformations required and the skills of the data engineer, the SQL dialect common to the
		Microsoft data stack (T-SQL) could play an important role.

		This module first introduces the scenarios where the move from an Azure SQL Database into Synapse databases
		could be useful, introduces briefly the two different types of SQL databases, and then focusses more deeply on
		the Synapse Analytics Serverless databases.
	</p>

	<ul>
		<li>When Azure SQL Databases reach their limits</li>
		<li>Provisioned versus Serverless Synapse Analytics databases</li>
		<li>Creating and accessing Serverless databases</li>
		<li>Using OPENROWSET for data access</li>
		<li>Creating external tables</li>
		<li>LAB: Querying data via Azure Synapse Analytics Serverless databases</li>
	</ul>

	<h4>Using Azure Synapse Analytics Provisioned SQL Pools</h4>
	<p>
		Since Serverless SQL Pools don't store data in a proprietary format, they lack features such as indexes, update
		statements etc. This is where
		Provisioned SQL Pools in Azure Synapse Analytics (formerly known as Azure Data
		Warehouse) can come to the rescue.
	</p>
	<ul>
		<li>Architecture of Provisioned SQL Pools</li>
		<li>Loading data via PolyBase</li>
		<li>Create Table as Select (CTAS)</li>
		<li>Setting up table distributions: Round Robin, Hash and Replicate</li>
		<li>Indexing options</li>
		<li>LAB: Loading and querying data in Provisioned SQL Pools</li>
	</ul>

	<h4>Getting started with Apache Spark</h4>
	<p>
		Although SQL is a very powerful language to access and manipulate data, it has its limitations. Complex data
		wrangling, advanced statistics or machine learning are ill-suited tasks for SQL.
		For this purpose Apache Spark is better suited. It's a divide-and-conquer framework for data access, transformation and
		querying which relies on programming languages such as Python and Scala.
	</p>

	<ul>
		<li>Introduction to Apache Spark</li>
		<li>Spark Cluster setup and configuration</li>
		<li>Getting started with Notebooks</li>
		<li>LAB: Getting started with Spark and Notebooks</li>
	</ul>

	<h4>Accessing data in Synapse Analytics Spark</h4>
	<p>
		Apache Spark doesn't have a proprietary data storage option, but consumes and produces regular files stored in Azure
		Storage. This module covers how to access and manipulate data stored in the Synapse Analytics data lake or other
		Azure storage locations from Synapse Analytics Spark.
	</p>

	<ul>
        <li>The Spark Session and Context objects</li>
		<li>Connecting to Azure Blob Storage and Azure Data Lake Storage Gen2</li>
		<li>Connecting to relation databases through JDBC or ODBC</li>
		<li>Loading and saving data in Spark using DataFrames</li>
		<li>An introduction to transforming data using PySpark</li>
		<li>Using Spark SQL to query and transform data</li>
		<li>LAB: Processing data on a Spark cluster</li>
	</ul>
	
	<h4>Introducing Delta Lake</h4>
	<p>
		Delta Lake is an optimized storage layer that provides the foundation for storing data and tables in a Lakehouse Platform. Delta lake
		is an open source platform that extends Parquet files with ACID transactions and metadata handling. 
		This chapter provides an introduction to Delta Lake and how it can be used to create a Lakehouse architecture.
	</p>
	<ul>
		<li>Introducing Delta Lake</li>
		<li>Creating Delta Tables</li>
		<li>Modifying data in a Delta Table</li>
		<li>Time Travel with Delta Tables</li>
		<li>Creating a Lake Database/Lakehouse in Azure Synapse Analytics</li>
		<li>Querying a Lake Database/Lakehouse using the Serverless SQL Pool</li>
	</ul>

	<h4>Azure Data Explorer</h4>
	<p>
		In between large volumes of historical, long-lived data stored in a data lake,
		and streams of short living events processed with Azure Stream Analytics, lives the challenge of
		working with large volumes of semi-structured telemetry and log data, where the analysis can have a
		longer latency that with event processing, but requires more historical information than what event processing
		technology can handle.
		For this kind of data processing Azure Data Explorer is the ideal tool
	</p>

	<ul>
		<li>Data Explorer architecture</li>
		<li>Ingesting data in Data Explorer</li>
		<li>Querying and visualizing data with the Kusto query language</li>
		<li>Accessing Data Explorer from Data Factory and Power BI</li>
	</ul>

	<h4>The role of the Power BI Service</h4>
	<p>
		The Power BI Service (or the Analysis Services engine directly) plays an important role in the modern data
		warehouse solution. This module describes briefly the Power BI Service architecture and how it integrates with
		Azure Synapse Analytics.
	</p>
	<ul>
		<li>Overview of Power BI</li>
		<li>Power BI/Analysis Services in the modern data warehouse architecture</li>
		<li>Link Synapse Analytics with Power BI</li>
	</ul>


</body>

</html>