<html xmlns:mso="urn:schemas-microsoft-com:office:office" xmlns:msdt="uuid:C2F41010-65B3-11d1-A29F-00AA00C14882">

<head>
  <title>Building and Deploying AI Solutions with Azure AI Foundry</title>
  <meta name="NumberOfDays" content="4" />
</head>

<body>
  <h2>Learning Goals</h2>
  <p>
    This course equips participants to develop, design and deploy AI solutions
    using Azure AI Foundry. You'll learn to collaborate on projects,
    manage resources, and use advanced AI techniques like prompt engineering,
    Retrieval Augmented Generation, and AI orchestration frameworks in Python like Prompt Flow and Langchain. The course
    also covers
    fine-tuning models for accuracy, ensuring responsible AI practices,
    and monitoring applications in production.
  </p>

  <h2>Target Audience</h2>
  <p>
    This course is designed for developers, data scientists, and AI Operators looking to leverage the full AI app
    development toolset provided by Azure AI Foundry.
    Basic understanding of Python is recommended.
  </p>
  <h2>Course Outline</h2>

  <h4>Introduction to Azure AI Foundry</h4>
  <p>
    Azure AI Foundry is a comprehensive platform that streamlines AI
    solution development and deployment. In this chapter, discover how to use hubs for building
    and testing AI solutions, projects for grouping and deploying AI apps, and tools
    for managing resources, all while ensuring responsible AI practices are followed.
  </p>
  <ul>
    <li>What is Azure AI Foundry?</li>
    <li>Collaborative AI Development</li>
    <li>Developing and Testing AI Solutions in Hubs</li>
    <li>Deploying AI Applications with Projects</li>
    <li>Managing Resources and Projects</li>
  </ul>

  <h4>Ready-to-Use AI Models with Azure AI Services</h4>
  <p>
    Azure AI services provides a comprehensive suite of out-of-the-box and customizable AI tools, APIs, and pre-trained
    models that
    detect sentiment, recognize speakers, understand pictures, etc.
    Azure AI Foundry brings together these services into a single, unified development environment.
  </p>
  <ul>
    <li>Azure AI Services Overview</li>
    <li>Azure AI Language</li>
    <li>Azure AI Vision</li>
    <li>Azure AI Speech</li>
    <li>Azure AI Document Intelligence</li>
  </ul>

  <h4>Azure OpenAI and Large Language Model Fundamentals</h4>
  <p>
    This module introduces Azure OpenAI and the GPT family of Large Language Models (LLMs). You'll
    learn about available LLM models, how to configure and use them in the Azure Portal, and the Transformer
    architecture behind models like GPT-4. The latest GPT models offer Function Calling, enabling connections
    to external tools, services, or code, allowing the creation of AI-powered Copilots. Additionally, you'll discover
    how Azure OpenAI provides a secure way to use LLMs without exposing your company's private data.
  </p>
  <ul>
    <li>Introducing OpenAI and Large Language Models</li>
    <li>The Transformer Model</li>
    <li>What is Azure OpenAI?</li>
    <li>Configuring Deployments</li>
    <li>Understanding Tokens</li>
    <li>LLM Pricing</li>
    <li>Azure OpenAI Chat Completions API</li>
    <li>Role Management: System, User and Assistant</li>
    <li>Azure OpenAI SDK</li>
    <li>Extending LLM capabilities with Function Calling</li>
    <li>LAB: Deploying and Using Azure OpenAI</li>
  </ul>

  <h4>Deploying AI Models</h4>
  <p>
    The cost and quality of your AI-powered app depend largely on
    your choice of AI model and how you deploy it. Learn about the
    available model catalog, featuring state-of-the-art Azure OpenAI
    models and open-source models from Hugging Face, Meta, Google,
    Microsoft, Mistral, and many more.
  </p>
  <ul>
    <li>Model Catalog Overview</li>
    <li>Model Benchmarks</li>
    <li>Selecting the Best Deployment Mode</li>
  </ul>

  <h4>Retrieving Semantically Related Data with Vector Search</h4>
  <p>
    Vector search is a powerful technique that allows you to retrieve semantically
    related data from large datasets such as company documents or databases.
    This chapter will teach you how vector search works and how it enables you to
    find relevant information without depending on exact keyword based search terms
    or language of the information in the dataset.
  </p>
  <ul>
    <li>Capture Semantic Meaning with Embeddings</li>
    <li>Vector Search</li>
    <li>Vector Search Design Considerations</li>
  </ul>

  <h4>Retrieval Augmented Generation with Azure AI Search</h4>
  <p>
    Azure AI Search enables the Retrieval Augmented Generation (RAG)
    design pattern, enhancing LLMs knowledge with your
    own company specific data. This chapter explores the RAG design pattern by incorporating Azure AI Search,
    into your LangChain/Prompt flow Python applications.
  </p>
  <ul>
    <li>What is Azure AI Search?</li>
    <li>Retrieval Augmented Generation with Prompt flow and LangChain</li>
    <li>Enhancing AI Models with your Own Data: Blog Storage, Azure SQL, OneLake...</li>
    <li>Hybrid Search with Semantic Reranking</li>
    <li>Use AI Enrichment to extract insights</li>
    <li>Fine-tuning vs RAG</li>
    <li>LAB: Chat with Azure OpenAI models using your own data</li>
  </ul>

  <h4>Developing AI powered Python Applications with Prompt Flow and LangChain</h4>
  <p>
    This chapter covers two powerfull Python libraries for AI applications: Prompt Flow and LangChain.
    Prompt Flow streamlines the design and deployment of prompt-based workflows, optimizing AI processes.
    LangChain simplifies building applications with LLMs through open-source components and quick integrations.
    Learn how to expose Python functions to LLMs as plugins, to let them interact with the outside world, enabling you
    to build your own Copilots!
  </p>
  <ul>
    <li>Introduction to Prompt Flow and LangChain</li>
    <li>Developing Prompt-Based Workflows</li>
    <li>Making your Python functions into LLM plugins</li>
    <li>Creating Simple LLM Applications</li>
    <li>Developing Chatbots</li>
    <li>Building Vector Stores and Retrievers</li>
    <li>Implementing Retrieval Augmented Generation (RAG) Applications</li>
    <li>Streamlining AI Processes</li>
  </ul>

  <h4>Prompt Engineering and Design Patterns</h4>
  <p>
    In this chapter, you'll explore advanced techniques allowing you to control the
    model's output, transforming generic responses into precise, valuable results. Additionally
    the chapter covers emerging design patterns in the field of Gen AI app development that help
    you increase quality of model responses and reduce costs.
  </p>
  <ul>
    <li>What is Prompt Engineering?</li>
    <li>Few-Shot Prompting</li>
    <li>Structured Query Generation</li>
    <li>Verifying Model responses with Hallucination Detection</li>
    <li>Saving costs with Semantic Caching</li>
  </ul>

  <h4>Testing and Moderating AI Models</h4>
  <p>
    How can you ensure an LLM provides relevant and coherent
    answers to users' questions using the correct info? How do you prevent an LLM from responding
    inappropriately? Discover the answers to these questions and more by exploring
    evaluation metrics in Azure AI Foundry and the Azure AI Content Safety
    Service.
  </p>
  <ul>
    <li>Ensuring Coherent and Relevant LLM Responses</li>
    <li>Utilizing Correct Information in AI Answers</li>
    <li>Preventing Inappropriate LLM Responses</li>
    <li>Exploring Custom Evaluation Metrics in Azure AI Foundry with Prompt flow</li>
    <li>Leveraging Azure AI Content Safety Service</li>
    <li>Enhancing AI Performance and Safety</li>
  </ul>


  <h4>Making your AI Apps Traceable</h4>
  <p>
    Ensuring your AI app behaves as expected doesn't end at deployment. It's
    crucial to monitor its interactions with users while it's running in
    production. Learn how Azure AI Foundry integrates with industry standards
    like OpenTelemetry to give you a clear and transparent view of your
    app's behavior.
  </p>
  <ul>
    <li>Monitoring AI App Interactions in Production</li>
    <li>Integrating OpenTelemetry with Azure AI Foundry</li>
    <li>Tracing and Debugging</li>
    <li>Capturing Model Calls and Latency Issues</li>
    <li>Setting Up Local Testing Environments</li>
  </ul>


  <h4>Fine-tuning AI Models with Azure AI Foundry</h4>
  <p>
    This chapter explores the advantages of fine-tuning pre-trained
    LLMs for higher accuracy and customized behavior compared to Retrieval Augmented
    Generation (RAG). While RAG offers dynamic updates and cost-effectiveness,
    fine-tuning provides superior precision for specialized tasks, making it ideal
    for achieving domain-specific results.
  </p>
  <ul>
    <li>Introduction to Fine-Tuning LLMs</li>
    <li>How to decide between Fine-Tuning and RAG?</li>
    <li>Using Task-Specific Data for Enhanced Performance</li>
    <li>Reducing Hallucinations with Fine-Tuning</li>
  </ul>
</body>

</html>